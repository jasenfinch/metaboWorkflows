---
title: "metaboWorkflows"
subtitle: "`r paste0('v',packageVersion('metaboWorkflows'))`"
author: "Jasen Finch"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  prettydoc::html_pretty:
    toc: true
    highlight: github
    theme: tactile
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

Metabolomics encompasses a wide range of techniques that includes both fingerprinting and profiling.
These require the application of a diverse range of processing algorithms, pre-treatment strategies and statistical analyses to extract biological information.
In turn, these often require numerous software packages, each providing just a small part of the metabolomics workflow.
However, these can be wrapped together to provide metabolomics analysis pipelines.

The aim of *metaboWorkflows* is to provide a high-level framework for metabolomics analysis pipelines that is simple to use for routine analyses, encompassing a wide range of metabolomics techniques.
*metaboWorkflows* breaks metabolomics analysis into three main sections:

* processing
* analysis
* annotation

Processing encompasses the signal processing aspect where an intensity matrix is constructed using algorithms specific to the metabolomics technique. 
Analysis encompasses the statistical analysis aspects where the data is analysed in the context of the biological question. This can include data pre-treatment, classification and feature selection steps.
Annotation can include assignment of molecular formulas or metabolite names to features extracted during processing.

These sections don't necessarily need to occur in that particular order for a given workflow. 
For instance, annotation could occur prior to certain analysis steps to make the results more interpretable in a biological context.

This document will first discuss how to use metaboWorkflows followed by more detailed discussion of each of the workflows that are available in the package.

## Basic Usage

The [*hrm*](https://github.com/jasenfinch/hrm) package provides utility for easy loading of the packages that can be required whilst using *metaboWorklows*.
This can be installed by running:

```{r hrm, eval=FALSE}
devtools::install_github('jasenfinch/hrm')
```

The packages can then easily be loaded using:

```{r hrmAttach,eval=FALSE}
hrm::hrmAttach()
```

```{r libraryLoad,include=FALSE}
library(metaboWorkflows)
library(metaboData)
library(binneR)
```

There are three main functions that are used within the *metaboWorkflows* package:

* `workflowParameters()` - allows the selection of the workflow to use and returns the relevant default parameters.
* `workflow()` - input files paths, sample information and workflow parameters to execute the workflow. 
* `restartWorkflow()` - used to restart a workflow after a check point stoppage or failure.

The available workflows within the package can be found by running:
```{r availbleWorkflows}
workflowParameters()
```

*metaboWorkflows* provides methods for the modification of parameters and extraction of results for the objects that are returned by these functions. However, because *metaboWorkflows* encompasses a number of different analysis packages, the user is referred to the documentation of the particular package for information on how to interact with their respective outputs.

More detailed discussion of each individual workflows provided by *metaboWorkflows* and the packages used by them can be found in their respective sections below.

### Sample Information

*metaboWorkflows* requires the of sample information (info) for the experimental run to be processed. 
This should be a tibble with the recommended column headers that include:

* _fileOrder_ - the file order in alphabetical order as returned by `list.files()`  
* _injOrder_ - the injection order of the samples during FIE-HRMS analysis
* _fileName_ - the sample file name
* _batch_ - the sample batch
* _batchBlock_ - the randomised block of the sample
* _name_ - the sample name
* _class_ - the sample class

The row orders of the info table should match the order in which the files paths are submitted to the `workflow()` function.
Further columns can be added to this file to denote different class labelling.

The file paths and sample info can be loaded for the example data using the following:

```{r files,message=FALSE}
files <- filePaths('FIE-HRMS','BdistachyonEcotypes') 
info <- runinfo('FIE-HRMS','BdistachyonEcotypes')

head(files)
info
```

### Parameter Selection

The default parameters for a particular workflow can be retrieved using the `workflowParameters()` function. 
For instance, the `FIE_HRMSfingerprinting` workflow can be retrieved using the following. 
The addition of the vector containing the file paths allows for automatic parameter detection.

```{r parameters}
parameters <- workflowParameters('FIE_HRMSfingerprinting',files = files)
parameters
```

The parameters can altered for a given workflow section using the following functions:

* `parametersProcessing()`
* `parametersAnalysis()`
* `parametersAnnotatation()`

For instance the processing parameters could be returned using:

```{r processingParameters}
parametersProcessing(parameters)
```

Then the scan numbers for spectral binning could be altered using:

```{r bin,eval=FALSE}
parametersProcessing(parameters) <- binParameters(scans = 5:11)
```

### Workflow

Executing a workflow is simple using the `workflow()` function.
The workflow can then be executed using the following.

```{r workflow, eval=FALSE}
analysis <- workflow(files,info,parameters)
```

The following functions can be used for extracting the workflow results of the relevant section:

* `resultsParameters()`
* `resultsProcessing()`
* `resultsAnalysis()`
* `resultsAnnotation()`

For instance to extract the processing results of a `Workflow` object:

```{r processingResults,eval=FALSE}
resultsProcessing(analysis)
```

#### Check Points

Some workflows can include check points where a workflow is intermediately stopped.
Reasons for this could include a step that requires a manual curation of data quality such as identifying outlier samples. These may need removing prior to more computationally intensive steps in the analysis.
After these check points the workflow can easily be restarted from the stop point using the following.

```{r continueWorkflow, eval=FALSE}
analysis <- restartWorkflow(analysis)
```

A checkpoint can be skipped simply by piping the output form the initial `workflow()` call directly into `restartWorkflow()` as shown below.

```{r completeWorkflow, eval=FALSE}
analysis <- workflow(files,info,parameters) %>%
  restartWorkflow()
```

#### Failure

There are likely to be times where a workflow will fail mid-analysis, potentially as a result of an unsuitably set parameter.
The included workflows are broken up into flag points to allow failures to be isolated and data processed from previous, potentially time consuming steps, to be returned.
The point of failure will also be printed to the console if an error occurs.
This allows the error to potentially be fixed then the workflow to be restarted from the point of failure using:

```{r restartWorkflow, eval=FALSE}
analysis <- restartWorkflow(analysis)
```

### Flags

Each workflow is made up of sections (flags) each with its own function. 
The flags for a given workflow can be retreived using the following:

```{r, showFlags}
wp <- workflowParameters('FIE_HRMSfingerprinting')
flags(wp)
```

Workflows can be customised using these flags.
Flags can be removed or added using `flags()`. For instance the following will remove the flags related to MF assignment:

```{r removeFlags}
flags(wp) <- flags(wp)[-(4:5)]
flags(wp)
```



